<!doctype html>
<html >
<head>
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />

    <!-- <link rel="stylesheet" type="text/css" href="template.css" /> -->
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/template.css" />

    <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />

    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <!-- <script type='text/javascript' src='menu/js/jquery.cookie.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.hoverIntent.minified.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.dcjqaccordion.2.7.min.js'></script> -->

    <!-- <link href="menu/css/skins/blue.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/graphite.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/grey.css" rel="stylesheet" type="text/css" /> -->
  
    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
        
  
    <!-- <script src="script.js"></script> -->
  
    <!-- <script src="jquery.sticky-kit.js "></script> -->
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.cookie.js'></script>
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.hoverIntent.minified.js'></script>
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/ryangrose/easy-pandoc-templates/948e28e5/css/elegant_bootstrap.css" rel="stylesheet" type="text/css" />
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
    <script src="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/script.js"></script>
  
    <script src="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/jquery.sticky-kit.js"></script>
    <meta name="generator" content="pandoc" />
  <title>Can be simpler</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>

    
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Can be simpler</span>
        <ul class="nav pull-right doc-info">
                            </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">

        <ul>
        <li><a href="#scared-of-ai">Scared of AI?</a></li>
        <li><a href="#lets-fix-ai.">Lets Fix AI.</a></li>
        <li><a href="#why-keys">Why “Keys?”</a></li>
        <li><a href="#less-but-better">Less, but Better</a></li>
        <li><a href="#manifolds">Manifolds</a></li>
        <li><a href="#feature-selection">Feature Selection</a></li>
        <li><a href="#sub-sampling">Sub-sampling</a></li>
        <li><a href="#measuring-dim">Measuring Dim</a></li>
        <li><a href="#low-dim-se">Low-Dim SE</a>
        <ul>
        <li><a href="#low-reach">Low Reach</a></li>
        <li><a href="#naturalness">Naturalness</a></li>
        <li><a href="#power-laws">Power Laws</a></li>
        </ul></li>
        <li><a href="#deep-learning">Deep Learning</a></li>
        <li><a href="#explanation">Explanation</a></li>
        </ul>

        </div>
      </div>
            <div class="span9">
            <h2 id="scared-of-ai">Scared of AI?</h2>
            <p><img src="img/deer.png" class="rimg150" /> Are you trapped like a deer in the headlights, frozen and terrified, by on-coming AI software? Well, then you are not a software engineer. When SE people are worried about software, they change it. Opaque systems are refactored into simpler and more maintainable code. Complexity is removed. Simplicity is installed.</p>
            <h2 id="lets-fix-ai.">Lets Fix AI.</h2>
            <p>Time to start practicing refactoring AI tools. Start with a simple object model then do some mixing-and-matching to implement an interesting range of functionality. Here’s what you have already (in under 500 lines of code):</p>
            <ul>
            <li>Explainable AI that generated tiny symbolic descriptions of data;</li>
            <li>Single and multi-objective reasoning so we can handle classification, regression, and optimization tasks;</li>
            <li>Semi-supervised learning where we guess most of the labels from a few samples.</li>
            <li>(Well… not really. Currently, this is more an aspirational statement. But a man’s reach should exceed his grasp, or what’s a heaven for?)</li>
            </ul>
            <p>There’s nothing magical about my code. Its certainly not the absolute optimum way to implementing all these tasks. In fact, the best thing I can say about this code is that it is short and easy to change. Treat it as a challenge of something you can do better than me. To practice refactoring AI tools, start with the few hundreds (or so) lines of code shown below. Implement them in your favorite language. Then try splashing our a little- mix and match some functionality. Pull out some code then add in something different. Test that addition. See if it works better than my stuff. Have fun with it!</p>
            <p>And when you do, be a software engineering. Try and come up with some underling object model that lets you handle not just your new idea, but also a whole product line of variations around that idea.</p>
            <h2 id="why-keys">Why “Keys?”</h2>
            <p>But why is it called <em>keys</em>? Well, the “key” to simpler AI is “keys”. Many results show that systems often exhibit the same effect <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>; i.e. a few key features control the rest. Just to say the obvious, for systems containing a few keys, then explanation and control is just a matter of running a handful of “what-if” queries across the keys. By exploiting the keys effect, it us possible to dramatically simplify the development of AI systems, as well as explaining their inner workings.</p>
            <h2 id="less-but-better">Less, but Better</h2>
            <p><img src="img/rams.png" class="rimg300" /> One of my heroes is the industrial designer Dieter Rams. Credited with hundreds of iconic products (everything from the Oral-B toothbrush to home audio equipment, Braun coffee makers to calculators), he inspired a generation of designers, including those designing Apple products.</p>
            <p>Rams firmly believes that “good design” involves as little design as possible which he describe as <em>“Weniger, aber besser”</em>; which translates to <em>less, but better</em>.</p>
            <p>So how can we be less but better with AI? What do we now know about AI that we did not know before? And how can we use that to simplify how we do AI?</p>
            <h2 id="manifolds">Manifolds</h2>
            <p><img src="img/cluster.png" class="rimg300" /> One lesson learned over the last two decades is that the best thing to do with lots of data is (carefully) throw most of it away <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. The reason most data mining works is that tables of data (with many columns) can be approximated by a lower dimensional <em>manifold</em> (i.e. a smaller number of attributes) without loss of signal. When data maps to a low dimensional manifold then there are fewer ways examples can differ. When differences are <em>less</em> then <em>continuity</em> increases between nearby examples. So when we cluster similar examples, then we do not to reason separately about each example. Rather, we just cluster the data and reason about a few examples per cluster.</p>
            <h2 id="feature-selection">Feature Selection</h2>
            <p>Since the underlying dimensional it of the data is usually very small, then we can safely (and usefully) superfluous attributes. This is called feature selection <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>:</p>
            <ul>
            <li>Weak (noisy) attributes should be removed since they can confuse inference.</li>
            <li><img src="img/tree.png" class="rimg300" /> And if we have many strong attributes that strongly associated with the class, then we should replace the many with just a few. Why? Well, if data is divided too much too soon, then sub-division become data starved and can make mistakes. <br clear=all></li>
            </ul>
            <h2 id="sub-sampling">Sub-sampling</h2>
            <p>Better yet, if we are trying to understand the difference between neighboring clusters, then there will very few differences. This means we can reduce the data even more using range selection. If we know one particular cluster is the goal cluster, then it is silly to divide numeric data unless it better predicts for that goal. For example:</p>
            <ul>
            <li><img src="img/fss.png" class="rimg500" /> Shown at right is a data set where we have two clusters for red and blue things. The blue:red class ratio is 5:3.</li>
            <li>The interesting ranges are those where blue:red is very different to 5:3. None such exist for “pres” or “skin” (so they can be reduced to one range).</li>
            <li>And when reasoning about this data, if we are trying to predict for red, then we could ignore everything except the two ranges where blue is more common than red (in “plas” and “insu”).</li>
            </ul>
            <h2 id="measuring-dim">Measuring Dim</h2>
            <p><img src="img/dim.png" class="rimg300" /> It is somewhat startling just how much little data is needed to approximate an entire data set. To see this, consider the following calculation. Suppose we (a)sort everyone’s neighbors by their distance, then plot how (b)count how many neighbors are found when we go from <em>r</em> to <em>2r</em> to <em>3r</em> etc (and find the maximum slope of the log-log form of that graph). If the data is linear (one-dimensional) or two dimensional, or three dimensional, then that slope will be 1,2,3, etc <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. For 80 data sets from the SE domains<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> and 40 from the standard UCI repository<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, those data sets have an underlying dimensionality less than half a dozen (for non-SE data) and even less for the SE examples.</p>
            <h2 id="low-dim-se">Low-Dim SE</h2>
            <p>Why is the SE data so less complex than data from other sources. That is a puzzle, but there are some promising explanations for why that is so:</p>
            <h3 id="low-reach">Low Reach</h3>
            <p><img src="img/zhang.png" class="rimg400" /> One explanation is that when code runs, it only visits the states approved by the combination of its internal logic – and this space need not be large. For example, Zhang et al. report that by generating tests only for the main branches in the code, even applications that process large cloud databases can be tested via just a few dozen inputs <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. <br clear=all></p>
            <h3 id="naturalness">Naturalness</h3>
            <p><img src="img/zipf.png" class="rimg300" /> Another explanation is <em>naturalness</em>; i.e. that prgramming language have the same repetitive properties as natural language, If a languages that adhere to Zipf’s Law, the frequency of a word is inversely proportional to its rank in the frequency table. That is, the most frequent word occurs around twice as often as the second most frequent word, three times as often as the third most frequent word, and so on. More generally, it means that the data extracted from that source is also (usually) very simple <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> [^devanbu16]. Hence, data collected in this way have usefully predictable statistical properties that can be captured in statistical language models and leveraged for software engineering tasks. <br clear=all></p>
            <h3 id="power-laws">Power Laws</h3>
            <p><img src="img/powerlaw.png" class="rimg300" /> As to power laws, distributing drawn from software data exhibit the long tailed distributions associated with power laws. To see why, suppose programmer2 most understands a small region of the code written by programmer1. That programmer Would tend to make most changes around that region. If programmer3 does the same for programmer2’s code, and programmer4 does the same for programmer3’s code then that, over time, that team would spend most of their time working on a tiny portion of the overall code base <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>.</p>
            <h2 id="deep-learning">Deep Learning</h2>
            <p>But what about deep learning, you might ask. Isn’t everything improved by the use of CPU-intensive modeling where no one can the model output? Well, maybe so, but maybe there is too much trust being placed in that one technology<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>. And even in that field, some researchers are reasoning that finding analogous to the above <a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>, i.e. that DL models can be dramatically simplified.</p>
            <p>XXX more here</p>
            <h2 id="explanation">Explanation</h2>
            <p>explanation is everything . better say a bduction. enoigh ymb9lsm. “A”s have subsets. How find? kakas 2002. use abduction as a design principle not a im-lentation principle. got to heiristics. got to data mining. clusers</p>
            <p>in anyc ase, what expmanations a re not is just running a trace. isually a different inferences</p>
            <section class="footnotes" role="doc-endnotes">
            <hr />
            <ol>
            <li id="fn1" role="doc-endnote"><p>T. Menzies, “Shockingly Simple:”KEYS" for Better AI for SE" in IEEE Software, vol. 38, no. 02, pp. 114-118, 2021. doi: 10.1109/MS.2020.3043014<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn2" role="doc-endnote"><p>Tim Menzies, David Owen, and Julian Richardson. 2007. The Strangest Thing About Software. Computer 40, 1 (January 2007), 54–60. DOI:https://doi.org/10.1109/MC.2007.37<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn3" role="doc-endnote"><p>T. Menzies, “Shockingly Simple:”KEYS" for Better AI for SE" in IEEE Software, vol. 38, no. 02, pp. 114-118, 2021. doi: 10.1109/MS.2020.3043014<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn4" role="doc-endnote"><p>Ron Kohavi and George H. John. 1997. Wrappers for feature subset selection. Artif. Intell. 97, 1–2 (Dec. 1997), 273–324. DOI:https://doi.org/10.1016/S0004-3702(97)00043-X<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn5" role="doc-endnote"><p>M. A. Hall and G. Holmes, “Benchmarking attribute selection techniques for discrete class data mining,” in IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 6, pp. 1437-1447, Nov.-Dec. 2003, doi: 10.1109/TKDE.2003.1245283.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn6" role="doc-endnote"><p>Elizaveta Levina and Peter J. Bickel. 2004. Maximum Likelihood estimation of intrinsic dimension. In “Proceedings of the 17th International Conference on Neural Information Processing Systems” (NIPS’04). MIT Press, Cambridge, MA, USA, 777–784.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn7" role="doc-endnote"><p>Yang, X., Chen, J., Yedida, R., Yu, Z., &amp; Menzies, T. (2021). Learning to recognize actionable static code warnings (is intrinsically easy). Empirical Software Engineering, 26(3), 1-24.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn8" role="doc-endnote"><p>Seacfraft repository of SE data:<br />
            https://zenodo.org/search?page=1&amp;size=20&amp;q=seacraft<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn9" role="doc-endnote"><p>Dua, D. and Graff, C. (2019). UCI Machine Learning Repository http://archive.ics.uci.edu/ml. Irvine, CA: University of California, School of Information and Computer Science.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn10" role="doc-endnote"><ol start="17" type="A">
            <li>Zhang, J. Wang, M. A. Gulzar, R. Padhye, and M. Kim, “Bigfuzz:Efficient fuzz testing for data analytics using framework abstraction,” in the 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2020, pp. 722–733</li>
            </ol>
            <a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></li>
            <li id="fn11" role="doc-endnote"><p>Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. 2012. On the naturalness of software. In Proceedings of the 34th International Conference on Software Engineering (ICSE ’12). IEEE Press, 837–847.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn12" role="doc-endnote"><p>Z. Lin and J. Whitehead, “Why power laws? an explanation from fine-grained code changes,” inProceedings of the 12th Working Conference on Mining Software Repositories, ser. MSR ’15. IEEE Press, 2015, p.68–75.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn13" role="doc-endnote"><p>R. Yedida and T. Menzies, “On the Value of Oversampling for Deep Learning in Software Defect Prediction,” in IEEE Transactions on Software Engineering, doi: 10.1109/TSE.2021.3079841.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn14" role="doc-endnote"><p>Frankle, Jonathan and Michael Carbin. “The lottery ticket hypothesis: Finding sparse, trainable neural networks.” 7th International Conference on Learning Representations, May 2019, New Orleans, Louisiana, ICLR, May 2019<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
            <li id="fn15" role="doc-endnote"><ol type="I">
            <li>Sucholutsky and M. Schonlau, “‘Less than one’-shot learning: Learning N classes from M &lt; N samples,” 2020, arXiv:2009.08449.</li>
            </ol>
            <a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></li>
            </ol>
            </section>
            </div>
    </div>
  </div>

  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
